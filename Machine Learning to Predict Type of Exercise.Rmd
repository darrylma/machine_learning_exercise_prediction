---
title: "Machine Learning to Predict Type of Exercise"
output: 
  html_document:
    keep_md: true
---
by Darryl Ma

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path = "./figure/")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)

library(lattice)
library(ggplot2)
library(caret)
library(rattle)
library(e1071)
library(randomForest)
library(gbm)
```

## Executive Summary

We set out to create a prediction model that would be able to predict if a barbell lift was being performed properly given measurements from accelerometers placed on the belt, forearm, arm, and dumbbell. We managed to create a random forest model that achieved 100% accuracy on both our validation and testing datasets. Additionally, we determined that in order to perform a barbell lift correctly, it is most important that the subject maintain his/her hips in the proper position as well as complete the full forearm motion. 

## Introduction

The goal of this project is to predict the manner in which a subject performed barbell lifts based on input from accelerometers placed on the belt, forearm, arm, and dumbbell. Six participants of age between 20-28 were asked to perform barbell lifts correctly and incorrectly in 5 different ways:

1. Class A: exactly according to the specification
2. Class B: throwing the elbows to the front
3. Class C: lifting the dumbbell only halfway 
4. Class D: lowering the dumbbell only halfway 
5. Class E: throwing the hips to the front

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. The datasets provided contain various quantitative measurements as each of these participants performed different types of barbell lifts. The aim is to categorize/predict how well an activity was performed based on these measurements. 

## Loading Data

Two datasets were provided, one training set and one testing set. The code below downloads and loads the datasets into R:

```{r}
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), 
                    header = TRUE, na.strings = c("NA", ""))
testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), 
                    header = TRUE, na.strings = c("NA", ""))
```

The training set contains `r dim(training)[1]` observations and `r dim(training)[2]` variables, and the testing set contains `r dim(testing)[1]` observations and `r dim(testing)[2]` variables. 

```{r}
str(training)
```

A quick glance at the list of variables below shows that a lot of the variables have NA values. Therefore our first objective is to remove any unnecessary variables that will not have an impact in the classification of the barbell lift class.

## Cleaning the Data

Firstly we selected to only keep variables which did not contain any NA values:

```{r}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
```

This reduced the number of variables to `r dim(training)[2]` and `r dim(testing)[2]` in the training and testing set, respectively. 

```{r}
training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]
```
Next, we noticed that the first 7 variables: X, username, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window and num_window, would probably not have a great impact on how we would classify different types of barbell lifts. Consequently, these first 7 variables were removed from the training and testing sets, further reducing the variable count to `r dim(training)[2]` and `r dim(testing)[2]` in the training and testing set, respectively.  

```{r}
str(training)
```

The final list of features above all appear to be possible factors in determining the final output variable, classe.

## Data Splitting

The training dataset was further split into a cross validation dataset to measure the effectiveness of each prediction model

```{r}
set.seed(1234)
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
training <- training[inTrain,]
validation <- training[-inTrain,]
```

## Prediction Models

A few prediction models were chosen to try and solve the classification problem: decision tree, random forest and gradient boosting method.

### Decision Tree

The following code uses the training set to fit a decision tree model, predicts the classe outcome on the validation set, and reports the confusion matrix and accuracy of the prediction model.

```{r}
rpartFit <- train(classe ~ ., data=training, method="rpart")
rpartPred <- predict(rpartFit, validation)
confusionMatrix(rpartPred, validation$classe)$table
rpart_acc <- confusionMatrix(rpartPred, validation$classe)$overall[1]
rpart_acc
fancyRpartPlot(rpartFit$finalModel)
```

From the output above, we see that the decision tree does not accurately predict the outcome, classe, very well only achieving `r round(rpart_acc[[1]]*100, digits=1)`% accuracy on the validation dataset (i.e. out of sample error of `r round((1-rpart_acc[[1]])*100, digits=1)`%). The confusion matrix further shows us that this model was particularly very poor at classifying class D, which is corroborated by the dendrogram, where there is no node that classifies any observation as class D. 

### Random Forest

The following code uses the training set to fit a random forest model, predicts the classe outcome on the validation set, and reports the confusion matrix and accuracy of the prediction model.

```{r}
rfFit <- train(classe ~ ., data=training, method="rf")
rfPred <- predict(rfFit, validation)
confusionMatrix(rfPred, validation$classe)$table
rf_acc <- confusionMatrix(rfPred, validation$classe)$overall[1]
rf_acc
```

From the output above, we see that the random forest accurately predicts the outcome, classe, very well, achieving `r round(rf_acc[[1]]*100, digits=1)`% accuracy on the validation set (i.e. out of sample error of `r round((1-rf_acc[[1]])*100, digits=1)`%). 

### Gradient Boost Model

The following code uses the training set to fit a gradient boost model, predicts the classe outcome on the validation set, and reports the confusion matrix and accuracy of the prediction model.

```{r}
gbmFit <- train(classe ~ ., data=training, method="gbm", verbose=FALSE)
gbmPred <- predict(gbmFit, validation)
confusionMatrix(gbmPred, validation$classe)$table
gbm_acc <- confusionMatrix(gbmPred, validation$classe)$overall[1]
gbm_acc
```

From the output above, we see that the gradient boost model accurately predicts the outcome, classe, very well, achieving `r round(gbm_acc[[1]]*100, digits=1)`% accuracy on the validation set (i.e. out of sample error of `r round((1-gbm_acc[[1]])*100, digits=1)`%). 

## Most Influential Variables

```{r}
imp <- data.frame(varImp(rfFit)[1])
top_ten_imp <- rownames(imp)[order(imp$Overall, decreasing=TRUE)[1:10]]
top_ten_imp
avg_by_classe <- aggregate(. ~ classe, training, mean)
avg_by_classe[,c("classe", top_ten_imp[1:3])]
```

The top 10 variables that influenced the random forest model in determining the classe outcome are printed above. As you can see `r top_ten_imp[1]`, `r top_ten_imp[2]`, and `r top_ten_imp[3]` were the top 3 most influential variables (refer to image below for a visual definition of pitch, yaw and roll). In general, we can conclude that being able to perform a barbell lift correctly has a lot to do with maintaining your hips in the proper position as well as making sure you complete full forearm motion. More specifically, when performing a barbell lift, you should be aiming for the following `r top_ten_imp[1]`, `r top_ten_imp[2]`, and `r top_ten_imp[3]` values:

- `r top_ten_imp[1]`: `r round(avg_by_classe[1,top_ten_imp[1]], digits=2)`
- `r top_ten_imp[2]`: `r round(avg_by_classe[1,top_ten_imp[2]], digits=2)`
- `r top_ten_imp[3]`: `r round(avg_by_classe[1,top_ten_imp[3]], digits=2)` 

![Graphical Depiction of Pitch, Yaw and Roll](figure/pitch_raw_roll.png)

## Conclusion

```{r}
testPred <- predict(rfFit, testing)
testPred
```

Of all the models, we found that the random forest achieved the highest accuracy, 100%. The accuracies achieved by the other models are listed below:

1. Decision Tree: `r round(rpart_acc[[1]]*100, digits=1)`%
2. Gradient Boost Model: `r round(gbm_acc[[1]]*100, digits=1)`%
3. Random Forest: `r round(rf_acc[[1]]*100, digits=1)`%

When the random forest model was used to predict the classe outcome for the testing set, it again achieved 100% accuracy, predicting the classe outcome correctly for all 20 observations. Additionally, we determined that in order to perform a barbell lift correctly, it is most important that the subject maintain his/her hips in the proper position as well as complete the full forearm motion.    

